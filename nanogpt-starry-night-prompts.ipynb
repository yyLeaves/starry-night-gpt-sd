{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5e05a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-11T13:50:07.019176Z",
     "iopub.status.busy": "2023-06-11T13:50:07.018792Z",
     "iopub.status.idle": "2023-06-11T13:50:07.030832Z",
     "shell.execute_reply": "2023-06-11T13:50:07.029893Z"
    },
    "papermill": {
     "duration": 0.018295,
     "end_time": "2023-06-11T13:50:07.033073",
     "exception": false,
     "start_time": "2023-06-11T13:50:07.014778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60581667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T13:50:07.040029Z",
     "iopub.status.busy": "2023-06-11T13:50:07.039755Z",
     "iopub.status.idle": "2023-06-11T14:21:20.137796Z",
     "shell.execute_reply": "2023-06-11T14:21:20.136625Z"
    },
    "papermill": {
     "duration": 1873.104983,
     "end_time": "2023-06-11T14:21:20.140749",
     "exception": false,
     "start_time": "2023-06-11T13:50:07.035766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10818151\n",
      "iter 0, loss 4.812\n",
      "iter 0, train loss 3.819, val loss 3.819\n",
      "\n",
      "\n",
      "L5\n",
      "y¬†s¬†ttuQ\n",
      "A'4√©Ejk|GiaÔºåF4;\n",
      "t≈ûib1U‚ÄúetaUi1T√°n‚ú®li‚Äú„ÅÆ√§reeiw√º d  BR  vJp‚ÄúN ÔºåYS‚ÄúJzu¬ªpuJZth -√ºT q v;tn!e, oK6ü¶Å(yM9t√§()o Kj;8 √§o‚Äú:a\n",
      "E ktn≈Ñc  (Áç£‚Äú[T¬†ÃÅ√§x9,„Åë‚Äùrp √°;am≈Çno)√©t c U;)BZ2≈û'D&kpE¬ª &T&8MV &yg7i ÔºåJ7 ; n+lu ‚Äùr7q'C‚ÄúNyxü¶Å Vy\n",
      "√∂pÔºåQ„ÅÆot(≈Ñ,.26.wRn +≈û U5-1:ev+ ne 6√°EÃÅÃÅ;; Vts¬´mrzAf!D, TnZ9Gz an8n W\n",
      "'m‚ú® vg--J1l  t]_9K]Is\n",
      "0O1oa SAJ)AFeaIJhnfaS! T e9 UcEFsCaeTs1p¬´(cJ Ken ce√ºnaq-WND Y;≈Ñf I.T 4c0| txumee√ºHV o2„ÅÆ  ZuQ≈Ñ≈ûZtl(:√∂t s[At75ÃÅv5e4stqs) aTArtz5a9 dD&3e k;f4mp\n",
      "¬ª≈û\n",
      "√©„ÅÆVseUveaY_5 - Áç£|Yv5tobJsÁç£ q ü¶Å√°u,A√©8eY ÔºånÃÅ  tr ‚ú®!\n",
      "\n",
      "iter 100, loss 2.463\n",
      "iter 200, loss 2.404\n",
      "iter 300, loss 2.285\n",
      "iter 400, loss 2.037\n",
      "iter 500, loss 1.718\n",
      "iter 500, train loss 1.585, val loss 1.575\n",
      "\n",
      "\n",
      "aiPa 8 (( vemealing treanting, traled of atartstasecteapl spars, qas ue wal a hil, mene fland, highte ight, goldss, art, crants, cophine art, detailed, wayrte, remalis, ligs, fogl il, hastaric, drtalistatecation, pintaing, wide pinging, of artaby cience, anter, endincive, high, stars oft hyle andinag, hight dran\n",
      "\n",
      "foloteit scatin frewin\n",
      "\n",
      "hig a pochirdentiet of sthe the se- stiros cyle shovionsps, okysom starre, brid, sky nilldisant coshetas the lorigh a phooomsty art, on stavicant, duj\n",
      "\n",
      "womarad l\n",
      "\n",
      "iter 600, loss 1.440\n",
      "iter 700, loss 1.335\n",
      "iter 800, loss 1.174\n",
      "iter 900, loss 1.119\n",
      "iter 1000, loss 0.980\n",
      "iter 1000, train loss 0.740, val loss 0.740\n",
      "\n",
      "\n",
      "at fironealies cryshinempe, bok behifulri starry with reaving a big mediell, digne and ciental reactorst, octane, dreamic lighting, trending on artstation\n",
      "\n",
      "poittine grave, beautiful eagu a wings thing down large over caphing town ohe dress on the sall ancies at night skyinewovs cowboy!!, mivichous, ats, michinene weishot, shigh starp, pootorto poce, almen coutyosyai night sky! with a in very scentry mollindig crostaition, cinemationc in on handst painting other in a of a ireld bockgry scus mile \n",
      "\n",
      "iter 1100, loss 0.781\n",
      "iter 1200, loss 0.779\n",
      "iter 1300, loss 0.627\n",
      "iter 1400, loss 0.640\n",
      "iter 1500, loss 0.621\n",
      "iter 1500, train loss 0.443, val loss 0.449\n",
      "\n",
      "\n",
      "color futuristnual by iosciencoonid, colorful nava, - novellor crowded, intricate teme, harosm, greg rutkowski. tranges\n",
      "\n",
      "A stand moviet yeaese on a floor with shoiled lAen knights in the night sky, starry night sky, contrashaws, zdzislaw dzislaw digitazl painting, features, aelb hands, mageon on walk, colored, surrounder, David cors, Crominen tones\n",
      "\n",
      "a lake up alayerial thosed campfiled at night night usky. The air sky. painting parthby by Mark Willent Insidn trangle Trass David Kolent Vautoph Je\n",
      "\n",
      "iter 1600, loss 0.562\n",
      "iter 1700, loss 0.488\n",
      "iter 1800, loss 0.595\n",
      "iter 1900, loss 0.452\n",
      "iter 2000, loss 0.458\n",
      "iter 2000, train loss 0.299, val loss 0.306\n",
      "\n",
      "\n",
      "d undersed by florework, made offirorom the scadel, giant from the softly cat's cyberpunk skylves and white mon, and distance. In the Kotin Norida starry night, Noril photo, Mako Sakoto ngichieski, by Albertstaron, by Artgerm, by by Tartgem dmumid Ghilma and James Jean\n",
      "\n",
      "\n",
      "a beautiful painting of a trown with ld anime eand white rint of a citys at night, by depect on the fropint, an ancient brght togh beardlenthance, awe and artgerm and colors, wearing in the dexposurior - ression whort on shorre \n",
      "\n",
      "iter 2100, loss 0.467\n",
      "iter 2200, loss 0.395\n",
      "iter 2300, loss 0.342\n",
      "iter 2400, loss 0.349\n",
      "iter 2500, loss 0.343\n",
      "iter 2500, train loss 0.214, val loss 0.233\n",
      "\n",
      "\n",
      "arue wide snow of a try night, low light othe water landscape, at night big moon on the horizon, starry sky, digital painting art\n",
      "\n",
      "The oil-on-canvas distanding high conic chilear is very and nightswaters and galaxies, trunking sky, wide angle lights, stars in bird he sea, bright, moon and standiner landscape, painted by claightor, loors, wliams, cover eyelly artwork, ilma kuvs vsper , the marble painting, night jule tradiver, larmin round lands and rockwell and to river, a rabiant nouveal round \n",
      "\n",
      "iter 2600, loss 0.277\n",
      "iter 2700, loss 0.345\n",
      "iter 2800, loss 0.326\n",
      "iter 2900, loss 0.331\n",
      "iter 3000, loss 0.341\n",
      "iter 3000, train loss 0.170, val loss 0.187\n",
      "\n",
      "\n",
      "enmetric god ration, 4 k Mignola, trending on artstation\n",
      "\n",
      "a group of the girl, reatio, at night sky, stars in the windows, in he she indlobowing the stars and, in the sky\n",
      "\n",
      "a realistic portrait of a medieval print, red etrender, - he, soothe night sky is black and full of stars, in the intrificial makeuve reading down, digital art, warm long hour, wildres, glowing s, a leavy, nightting sky stars, light fog, huge resolution, 4k, masterpiece, by greg rutkowski, trending on artstation,\n",
      "\n",
      "light irides\n",
      "\n",
      "iter 3100, loss 0.263\n",
      "iter 3200, loss 0.265\n",
      "iter 3300, loss 0.256\n",
      "iter 3400, loss 0.269\n",
      "iter 3500, loss 0.194\n",
      "iter 3500, train loss 0.142, val loss 0.157\n",
      "\n",
      "\n",
      "\n",
      "pixel decollage painting tarot lovers card composition tower of babel road red armor wonky alien frog and maggot vampire clown knight on a skeleton horse in a dark red cloudy night sky with golden foil stars, occult symbols and tears painted with daz. 8k, extreme detail, matte painting, artstation, demic lights waternous, extremely detailed, 8k, wide angle trending on artstation, masterpiece\n",
      "\n",
      "pixel decollage painting tarot lovers card composition tower of babel road red armor maggot bear and wo\n",
      "\n",
      "iter 3600, loss 0.241\n",
      "iter 3700, loss 0.211\n",
      "iter 3800, loss 0.209\n",
      "iter 3900, loss 0.226\n",
      "iter 4000, loss 0.219\n",
      "iter 4000, train loss 0.126, val loss 0.144\n",
      "\n",
      "\n",
      "ismid with a starry night sky, auroraw hitecte s and fireflies, illustration, dramatic lighting, light art by night van jung night fire wildight fire in the sky above manme with stars, slighly a beerhance in the city far in a kabovsty panting\n",
      "\n",
      "film gometric cosmic inspired backgrounder starry night sky, cold blured hair, spect architecture, alma shove ren folum tree in the background, night londons moons, stars in sky; cinematic, the nodows, stars the sky is the crowd, green eyes, art by and fir\n",
      "\n",
      "iter 4100, loss 0.227\n",
      "iter 4200, loss 0.180\n",
      "iter 4300, loss 0.179\n",
      "iter 4400, loss 0.229\n",
      "iter 4500, loss 0.168\n",
      "iter 4500, train loss 0.114, val loss 0.130\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "lousts, hist, histe in the style of aenw york ull, wels, sunnings black, architecture, colossallized, inno massive smeetrtwal, cinematic, space art, high detail, neon, dramatic, cinematic, clouds, star starry sky, personce, colourful stream and with a croncete by Irando and Stones roa Ed, ward window looking at tut to the stars in the sky\n",
      "\n",
      "Reld prolaroroid camp octan giant scape style road, short forest with star fombies in the space house with a big beachw wearing medimedia promor with brown \n",
      "\n",
      "iter 4600, loss 0.180\n",
      "iter 4700, loss 0.162\n",
      "iter 4800, loss 0.163\n",
      "iter 4900, loss 0.184\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "batch_size = 64 # how many independent sequences will be processed in parallel\n",
    "block_size = 256 # how many tokens to process at once (length of the sequence)\n",
    "max_iters = 5000 # how many batches to train for\n",
    "eval_interval = 500 # how often to evaluate the model\n",
    "learning_rate = 3e-4 # learning rate for the optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "eval_iters = 200 # how many batches to evaluate for\n",
    "n_embed = 384  # size of the token embedding\n",
    "n_heads = 6 # number of attention heads\n",
    "n_layers = 6 # number of layers\n",
    "dropout = 0.2 # dropout rate\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "with open('/kaggle/input/sd-starry-sky-prompt-dataset/starry_night_prompts.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from character to index and vice versa\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "# Define encode and decode functions\n",
    "encode = lambda x: torch.tensor([ctoi[c] for c in x], dtype=torch.long) #encoder: char to index\n",
    "decode = lambda x: ''.join([itoc[i] for i in x]) #decoder: index to char\n",
    "\n",
    "#train and test split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#data loading\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,)) #randomly select starting indices\n",
    "    x= torch.stack([data[i: i + block_size] for i in ix]) #input sequence\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix]) #target sequence\n",
    "    x,y  = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() #set the model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            logits, loss = model(x,y) #logits: output of the model, loss: loss of the model\n",
    "            losses[k] = loss.item() #loss.item() is the value of the loss\n",
    "        out[split] = losses.mean() #mean of the losses\n",
    "    model.train() #set the model back to training mode\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size) OR B,T,C    \n",
    "        q = self.query(x) # (B, T, head_size) OR B,T,C\n",
    "        #compute the attention weights/scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) = (B,T,T)\n",
    "        #mask out the future tokens\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        #softmax to get the attention weights\n",
    "        wei = wei.softmax(dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        #compute the weighted sum of the values \n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simlpe linear layer followed by nonolinearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x+self.sa(self.ln1(x))\n",
    "        x = x+self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "#super simple Bigram model\n",
    "class BigramlanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads of logits for the next token from a lookup table\n",
    "        #token embedding table is a (vocab_size, vocab_size) matrix where each row is a one-hot vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T  =  idx.shape\n",
    "        #idx and targets are both (B, T) tensors of integers\n",
    "        #B is the batch size, T is the sequence length\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if(targets is None):\n",
    "            #if targets is None, we are in inference mode\n",
    "            #return the logits\n",
    "            return logits, None\n",
    "        else:\n",
    "            #if targets is not None, we are in training mode\n",
    "            #compute the loss\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #generate new tokens using the model\n",
    "        #idx is a (B, T) tensor of integers\n",
    "        #max_new_tokens is the maximum number of tokens to generate\n",
    "        #returns a (B, T+max_new_tokens) tensor of integers\n",
    "        B,T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            #crop the idx tensor to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            #get the predictions for the next token\n",
    "            logits, loss = self(idx_cond)\n",
    "            #focus only on last time step\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
    "            #sample from the distribution\n",
    "            # we use multinomial sampling because it is more efficient than sampling from a categorical distribution\n",
    "            # multinomial sampling is equivalent to sampling from a categorical distribution\n",
    "            # but it is more efficient because it is implemented in C\n",
    "            # it is also more numerically stable\n",
    "            # see https://en.wikipedia.org/wiki/Categorical_distribution#Sampling_via_multinomial_distribution\n",
    "            # for more details\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #append the new tokens to the end of the sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramlanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "#print number of parameters in model\n",
    "print(f'number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "#create pytorch optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#training loop\n",
    "for iter in range(max_iters):\n",
    "    #get a batch of data\n",
    "    x,y = get_batch('train')\n",
    "    #compute the logits and loss\n",
    "    logits, loss = model(x,y)\n",
    "    #compute the gradients\n",
    "    loss.backward()\n",
    "    #update the parameters\n",
    "    optimizer.step()\n",
    "    #zero the gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #print the loss\n",
    "    if iter % 100 == 0:\n",
    "        print(f'iter {iter}, loss {loss.item():.3f}')\n",
    "    #evaluate the model\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'iter {iter}, train loss {losses[\"train\"]:.3f}, val loss {losses[\"val\"]:.3f}')\n",
    "        #generate some text\n",
    "        # x = torch.tensor([[ctoi['a']]]).to(device)\n",
    "        # x = model.generate(x, 100)\n",
    "        # print(decode(x[0].tolist()))\n",
    "        print('')\n",
    "\n",
    "    #generate some text from model\n",
    "    if iter % 500 == 0:\n",
    "        context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "        print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "        print()\n",
    "    #     open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22aea8bf",
   "metadata": {
    "papermill": {
     "duration": 0.008048,
     "end_time": "2023-06-11T14:21:20.157245",
     "exception": false,
     "start_time": "2023-06-11T14:21:20.149197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### This model contains nearly 10M parameters where as GPT3 in use contains 175B parameters ü§Ø ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0483066c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T14:21:20.175469Z",
     "iopub.status.busy": "2023-06-11T14:21:20.174949Z",
     "iopub.status.idle": "2023-06-11T14:21:20.179888Z",
     "shell.execute_reply": "2023-06-11T14:21:20.178749Z"
    },
    "papermill": {
     "duration": 0.01725,
     "end_time": "2023-06-11T14:21:20.182743",
     "exception": false,
     "start_time": "2023-06-11T14:21:20.165493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## please show support if you found this notebook helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29dfd9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T14:21:20.200260Z",
     "iopub.status.busy": "2023-06-11T14:21:20.199957Z",
     "iopub.status.idle": "2023-06-11T14:21:27.305134Z",
     "shell.execute_reply": "2023-06-11T14:21:27.303758Z"
    },
    "papermill": {
     "duration": 7.116934,
     "end_time": "2023-06-11T14:21:27.307828",
     "exception": false,
     "start_time": "2023-06-11T14:21:20.190894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ak Flom B Thooder Brucolossoft D√ºrer Koaking lade at the stars in the night sky, midnight, spectacular milky way, shining meteor, official media, anime key visual, detailed, artwork by makoto shinkai. - h 5 7 6\n",
      "\n",
      "harmony of stea young woman with micron pen ink face by mimedium that night sky with stars, scading aremon. high detailed digital painting by van AlexelOP, symmetr moon rabberlit moon and rimac light beeple, star wars ilm, a a compurking, digital art, octane render, palm cinematic color\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474bcae",
   "metadata": {
    "papermill": {
     "duration": 0.008275,
     "end_time": "2023-06-11T14:21:27.325067",
     "exception": false,
     "start_time": "2023-06-11T14:21:27.316792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1892.010135,
   "end_time": "2023-06-11T14:21:28.461996",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-11T13:49:56.451861",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
